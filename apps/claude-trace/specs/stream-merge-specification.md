# Stream Chunk Merge Specification

## Overview

This specification defines how to merge individual streaming chunks from `.stream.jsonl` files back into consolidated entries in the main JSONL log file.

## Current State Analysis

### Main Log Format (`log-YYYY-MM-DD-HH-MM-SS.jsonl`)

```json
{
	"request": {
		"timestamp": 1751914728.311,
		"method": "POST",
		"url": "https://api.anthropic.com/v1/messages?beta=true",
		"headers": { "request-id": "req_011CQtAuwg7WapP4CKP293Vt" },
		"body": { "stream": true, "model": "claude-3-5-haiku-20241022" }
	},
	"response": {
		"timestamp": 1751914730.45,
		"status_code": 200,
		"headers": { "content-type": "text/event-stream; charset=utf-8" },
		"body_raw": "event: message_start\ndata: {...}\n\nevent: content_block_delta\ndata: {...}\n\n"
	},
	"logged_at": "2025-07-07T18:58:50.644Z"
}
```

### Enhanced Main Log Format with Streaming Details

The main JSONL file now includes complete streaming metadata for streaming responses:

```json
{
	"request": {
		/* standard request data */
	},
	"response": {
		"timestamp": 1751914730.45,
		"status_code": 200,
		"headers": { "content-type": "text/event-stream; charset=utf-8" },
		"body_raw": "complete_sse_content",
		"streaming_details": {
			"chunk_count": 8,
			"first_chunk_timestamp": 1751914730.451,
			"last_chunk_timestamp": 1751914730.522,
			"total_duration_ms": 71,
			"chunks": [
				{
					"sequence": 1,
					"timestamp": 1751914730.451,
					"event_type": "message_start",
					"data": { "type": "message_start", "message": { "id": "msg_01JpwQ4pVf9ZFzrysw7a9vK8" } },
					"chunk_timing_ms": 0
				}
			]
		}
	},
	"logged_at": "2025-07-07T18:58:50.644Z"
}
```

## Grouping Strategy

### Primary Correlation Method: Request ID Matching

- **Direct Correlation**: Match stream chunks to main log entries using `request_id` field
- **Reliability**: 100% accurate correlation, no ambiguity
- **Implementation**: Stream chunks include the same request ID generated by `generateRequestId()` in interceptor.ts:228

### Secondary Correlation Method: Message ID Extraction

- Extract message ID from `message_start` SSE event in chunk_data
- Match against reconstructed response content
- **Use Case**: Cross-validation and debugging
- **Fallback**: If request ID is missing (legacy data)

### Legacy Correlation Method: Timestamp-Based Grouping

- Group stream chunks by proximity to main log response timestamps
- **Tolerance Window**: Â±2 seconds from response timestamp
- **Use Case**: Processing logs from older versions without request_id field
- **Limitation**: Potential ambiguity with concurrent requests

## Merged Output Format

### Target Structure

```json
{
	"request": {
		/* original request data */
	},
	"response": {
		"timestamp": 1751914730.45,
		"status_code": 200,
		"headers": { "content-type": "text/event-stream; charset=utf-8" },
		"body_raw": "event: message_start\ndata: {...}\n\n...",
		"streaming_details": {
			"chunk_count": 10,
			"first_chunk_timestamp": 1751914730.451,
			"last_chunk_timestamp": 1751914730.522,
			"total_duration_ms": 71,
			"reconstructed_from_chunks": true,
			"chunks": [
				{
					"sequence": 1,
					"timestamp": 1751914730.451,
					"event_type": "message_start",
					"data": { "type": "message_start", "message": { "id": "msg_01JpwQ4pVf9ZFzrysw7a9vK8" } },
					"chunk_timing_ms": 0
				},
				{
					"sequence": 2,
					"timestamp": 1751914730.455,
					"event_type": "content_block_start",
					"data": { "type": "content_block_start", "index": 0 },
					"chunk_timing_ms": 4
				}
			]
		}
	},
	"logged_at": "2025-07-07T18:58:50.644Z"
}
```

### Field Specifications

#### `streaming_details` Object

- **chunk_count**: Total number of chunks merged
- **first_chunk_timestamp**: Timestamp of first chunk
- **last_chunk_timestamp**: Timestamp of last chunk
- **total_duration_ms**: Duration from first to last chunk
- **reconstructed_from_chunks**: Boolean flag indicating reconstruction
- **chunks**: Array of processed chunk objects

#### `chunks` Array Elements

- **sequence**: Order of chunk in stream (1-indexed)
- **timestamp**: Original chunk timestamp
- **event_type**: Extracted SSE event type
- **data**: Parsed JSON data from SSE event
- **chunk_timing_ms**: Milliseconds from first chunk

## Chunk Reconstruction Algorithm

### Phase 1: Request Grouping

```typescript
interface StreamChunk {
	timestamp: number;
	request_id: string;
	chunk_data: string;
	event_type: string;
	logged_at: string;
}

interface RequestGroup {
	request_id: string;
	chunks: StreamChunk[];
	mainLogEntry?: any;
}

function groupChunksByRequestId(streamChunks: StreamChunk[]): Map<string, RequestGroup> {
	const groups = new Map<string, RequestGroup>();

	for (const chunk of streamChunks) {
		if (!groups.has(chunk.request_id)) {
			groups.set(chunk.request_id, {
				request_id: chunk.request_id,
				chunks: [],
			});
		}
		groups.get(chunk.request_id)!.chunks.push(chunk);
	}

	// Sort chunks within each group by timestamp
	for (const group of groups.values()) {
		group.chunks.sort((a, b) => a.timestamp - b.timestamp);
	}

	return groups;
}
```

### Phase 2: Chunk Parsing

```typescript
interface ParsedChunk {
	sequence: number;
	timestamp: number;
	eventType: string;
	data: any;
	chunkTimingMs: number;
	request_id: string;
}

function parseSSEChunk(chunk: StreamChunk, sequence: number, firstTimestamp: number): ParsedChunk {
	const lines = chunk.chunk_data.split("\n");
	const eventLine = lines.find((line) => line.startsWith("event: "));
	const dataLine = lines.find((line) => line.startsWith("data: "));

	const eventType = eventLine?.replace("event: ", "").trim() || "unknown";
	const data = dataLine ? JSON.parse(dataLine.replace("data: ", "")) : {};

	return {
		sequence,
		timestamp: chunk.timestamp,
		request_id: chunk.request_id,
		eventType,
		data,
		chunkTimingMs: Math.round((chunk.timestamp - firstTimestamp) * 1000),
	};
}
```

### Phase 3: Response Reconstruction

```typescript
function reconstructResponse(chunks: ParsedChunk[]): string {
	return chunks.map((chunk) => `event: ${chunk.eventType}\ndata: ${JSON.stringify(chunk.data)}\n\n`).join("");
}
```

### Phase 4: Chunk Filtering and Validation

```typescript
function filterAndValidateChunks(chunks: StreamChunk[]): StreamChunk[] {
	// Filter out ping events (noise)
	const filteredChunks = chunks.filter((chunk) => !isPingEvent(chunk.chunk_data));

	// Validation checks
	validateStreamCompleteness(filteredChunks);
	validateContentBlockBalance(filteredChunks);
	validateEventSequence(filteredChunks);
	validateTimestampMonotonicity(filteredChunks);

	return filteredChunks;
}

function isPingEvent(chunkData: string): boolean {
	return chunkData.includes('event: ping\ndata: {"type": "ping"}');
}
```

**Validation Criteria:**

- **Request ID Consistency**: All chunks have matching request_id
- **Stream Completeness**: Verify message_start and message_stop events exist
- **Content Block Balance**: Ensure content_block_start/stop pairs are balanced
- **Event Sequence**: Check for expected SSE event ordering (excluding ping events)
- **Timestamp Monotonicity**: Validate chunks are in chronological order
- **Ping Event Filtering**: Remove ping events before processing (they add no conversational value)

## Error Handling

### Incomplete Streams

- **Missing message_start**: Mark as `incomplete_stream_start`
- **Missing message_stop**: Mark as `incomplete_stream_end`
- **Orphaned chunks**: Stream chunks with request_id that has no matching main log entry
- **Missing main log**: Main log entry missing for chunks with valid request_id

### Corrupted Data

- **Invalid JSON in data**: Store as `corrupted_data` with raw text
- **Malformed SSE**: Store as `malformed_sse` with original chunk_data
- **Timestamp inconsistencies**: Flag as `timing_anomaly`
- **Request ID mismatches**: Chunks with inconsistent request_id values

### Processing Errors

```json
{
	"error": {
		"type": "stream_reconstruction_error",
		"message": "Failed to parse chunk 3 of 10",
		"request_id": "req_1751914728_abc123def",
		"chunk_sequence": 3,
		"original_chunk": {
			/* raw chunk data */
		},
		"error_details": "Invalid JSON in data field"
	}
}
```

## Real-time Processing Strategy

Stream chunks are processed and merged in real-time during response processing within the interceptor. This approach provides immediate availability of complete streaming metadata without requiring separate files or post-processing steps.

**Implementation Benefits**:

- Single log file with complete information
- Immediate availability of streaming metadata
- No separate correlation logic needed
- Backward compatibility with existing tools

## Enhanced File Output

### Main JSONL Log File

- **Naming**: `log-YYYY-MM-DD-HH-MM-SS.jsonl` (existing format)
- **Format**: Standard JSONL with enhanced `streaming_details` for streaming responses
- **Benefits**: Single file contains complete information, no separate correlation needed

## Implementation Phases

### Phase 1: Real-time Stream Enhancement (Week 1)

- Modify interceptor to build streaming_details during response processing
- Implement real-time chunk parsing and SSE reconstruction
- Add ping event filtering during chunk accumulation
- Create real-time validation rules

### Phase 2: Integration & Testing (Week 2)

- Update response object creation with streaming_details
- Implement error handling for streaming failures
- Add performance monitoring for real-time processing
- Test with various stream sizes and patterns

### Phase 3: Enhancement & Optimization (Week 3)

- Update HTML generator for new streaming metadata
- Optimize memory usage during real-time processing
- Document implementation and edge cases
- Performance tuning and validation

## Success Criteria

1. **Real-time Processing**: Complete streaming metadata generation during response processing
2. **Performance**: < 5ms additional latency per chunk, < 10ms completion overhead
3. **Memory Efficiency**: < 10MB memory usage per active stream
4. **Data Quality**: Ping events filtered, complete event parsing and validation
5. **Compatibility**: Enhanced log format works with existing analysis tools
6. **Reliability**: Graceful handling of stream interruptions and malformed data
7. **Single Source**: All streaming information contained in main JSONL file

## Required Interceptor Changes

To implement real-time streaming metadata generation, the following code modifications are needed:

### 1. Enhance `handleStreamingResponse()` for real-time processing:

```typescript
private async handleStreamingResponse(response: Response): Promise<{
  body_raw: string;
  streaming_details: StreamingDetails
}> {
    const reader = response.body?.getReader();
    const decoder = new TextDecoder();
    let fullContent = "";
    const chunks: ParsedChunk[] = [];
    const startTime = Date.now() / 1000;

    if (reader) {
        try {
            let sequence = 1;
            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value, { stream: true });
                fullContent += chunk;

                // Parse and accumulate chunk details (skip ping events)
                if (!this.isPingEvent(chunk)) {
                    const parsedChunk = this.parseSSEChunk(chunk, sequence++, startTime);
                    chunks.push(parsedChunk);
                }
            }
        } finally {
            reader.releaseLock();
        }
    }

    // Build streaming details
    const streaming_details = this.buildStreamingDetails(chunks, startTime);

    return { body_raw: fullContent, streaming_details };
}
```

### 2. Add real-time chunk parsing:

```typescript
private parseSSEChunk(chunkData: string, sequence: number, startTime: number): ParsedChunk {
    const lines = chunkData.split('\n');
    const eventLine = lines.find(line => line.startsWith('event: '));
    const dataLine = lines.find(line => line.startsWith('data: '));

    const eventType = eventLine?.replace('event: ', '').trim() || 'unknown';
    const data = dataLine ? JSON.parse(dataLine.replace('data: ', '')) : {};
    const timestamp = Date.now() / 1000;

    return {
        sequence,
        timestamp,
        event_type: eventType,
        data,
        chunk_timing_ms: Math.round((timestamp - startTime) * 1000)
    };
}

private isPingEvent(chunk: string): boolean {
    return chunk.includes('event: ping\ndata: {"type": "ping"}');
}
```

### 3. Update response creation to include streaming details:

- Modify response object creation in main fetch handler
- Include `streaming_details` only for streaming responses
- Remove separate `logStreamChunk()` and `writeStreamEvent()` methods

## Future Enhancements

1. **Advanced Analytics**: Detailed latency analysis, chunk distribution metrics, and performance profiling
2. **Optimization**: Memory optimization for large streams, chunk compression, and deduplication
3. **Monitoring**: Real-time processing metrics, stream health monitoring, and performance dashboards
4. **Visualization**: Enhanced HTML reports with interactive stream timing charts and performance analysis
